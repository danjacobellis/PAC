{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08b213f-5da6-48b2-b0a2-d75998f5e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from io import BytesIO\n",
    "import torch\n",
    "import torchaudio\n",
    "from spauq.core.metrics import spauq_eval\n",
    "import zlib\n",
    "import numpy as np\n",
    "from dance.audio import RateDistortionAutoEncoder\n",
    "from transformers import EncodecModel, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17bb9b35-475c-4efc-a1c7-8f7785abd3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f533dfe491b94b4593ccd5e0b3d9a532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce477570f9a84c94889a579a2f2ba6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"danjacobellis/aria_ea_audio_preprocessed\",split='validation').with_format(\"torch\").select(range(0,1200,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e12a56f1-8c65-4e3f-b506-0e6b220482a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "encodec_model = EncodecModel.from_pretrained(\"facebook/encodec_48khz\")\n",
    "encodec_processor = AutoProcessor.from_pretrained(\"facebook/encodec_48khz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a27345a-cb55-4b20-a095-dc7b499fb86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "361f09bf-579e-4d0e-9f7b-66f534ea6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=48000\n",
    "audio = sample['audio'].permute(1,0)\n",
    "reference = []\n",
    "recovered = []\n",
    "size_bytes = []\n",
    "for c in range(0,7,2):\n",
    "    channels = audio[c:c+2]\n",
    "    if channels.shape[0] != 2:\n",
    "        channels = torch.cat([channels,channels])\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(raw_audio=channels, sampling_rate=processor.sampling_rate, return_tensors='pt')\n",
    "        encoder_outputs = encodec_model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])\n",
    "        size_bytes.append(6.0*10.0*torch.tensor(encoder_outputs.audio_codes.shape).prod().item()/8)\n",
    "        audio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]\n",
    "        audio_values = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_values\n",
    "        reference.append(inputs['input_values'][0].detach())\n",
    "        recovered.append(audio_values[0].detach())\n",
    "reference = torch.cat(reference)[0:7]\n",
    "recovered = torch.cat(recovered)[0:7]\n",
    "eval_output = spauq_eval(reference=reference,estimate=recovered,fs=fs)\n",
    "sample['encodec48_SSR'] = eval_output['SSR']\n",
    "sample['encodec48_SRR'] = eval_output['SRR']\n",
    "sample['encodec48_cr'] = 3*audio.numel()/sum(size_bytes) # 24 bit audio\n",
    "sample['encodec48_cr'] = (8/7)*sample['encodec48_cr'] # only 7 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30ca6ce0-813c-425f-9fb5-358696b3aa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': tensor([[-0.4036, -0.0082,  0.6006,  ..., -0.0058, -0.1146, -0.0014],\n",
       "         [-0.2036,  0.1201, -0.0901,  ...,  0.0209, -0.1372, -0.0168],\n",
       "         [-0.0499,  0.0487, -0.2162,  ...,  0.0174, -0.2180,  0.1571],\n",
       "         ...,\n",
       "         [-0.0534, -0.0371, -0.0210,  ..., -0.0160, -0.0840,  0.0404],\n",
       "         [-0.0616, -0.0395, -0.0203,  ..., -0.0121, -0.0539,  0.0353],\n",
       "         [-0.0645, -0.0397, -0.0230,  ..., -0.0132, -0.1023,  0.0271]]),\n",
       " 'seq_name': 'loc2_script3_seq3_rec2',\n",
       " 'encodec48_SSR': 4.986122185749759,\n",
       " 'encodec48_SRR': -4.835965941433608,\n",
       " 'encodec48_cr': 114.28571428571428}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df844d0-e276-4b03-a208-6c997a06a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_q9(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    config = torchaudio.io.CodecConfig(qscale=9)\n",
    "    size_bytes = []\n",
    "    recovered = []\n",
    "    for channel in audio:\n",
    "        with BytesIO() as f:\n",
    "            torchaudio.save(f, channel.unsqueeze(0), format=\"mp3\", sample_rate=fs, compression=config)\n",
    "            size_bytes.append(len(f.getvalue()))\n",
    "            f.seek(0)\n",
    "            recovered.append(torchaudio.load(f,format=\"mp3\")[0])\n",
    "    recovered = torch.cat(recovered)\n",
    "    eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "    sample['mp3_q9_SSR'] = eval_output['SSR']\n",
    "    sample['mp3_q9_SRR'] = eval_output['SRR']\n",
    "    sample['mp3_q9_cr'] = 3*audio.numel()/sum(size_bytes) # 24 bit audio\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a0870c-701a-4a60-b39c-468666e79cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8706763cca5428aa260b31cb9d6dd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/spauq/core/preprocessing.py:325: UserWarning: No forgive_mode specified, defaulting to `none`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(mp3_q9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6548fb-29cd-4162-89a2-c8c4fea5a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_q1(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    config = torchaudio.io.CodecConfig(qscale=1)\n",
    "    size_bytes = []\n",
    "    recovered = []\n",
    "    for channel in audio:\n",
    "        with BytesIO() as f:\n",
    "            torchaudio.save(f, channel.unsqueeze(0), format=\"mp3\", sample_rate=fs, compression=config)\n",
    "            size_bytes.append(len(f.getvalue()))\n",
    "            f.seek(0)\n",
    "            recovered.append(torchaudio.load(f,format=\"mp3\")[0])\n",
    "    recovered = torch.cat(recovered)\n",
    "    eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "    sample['mp3_q1_SSR'] = eval_output['SSR']\n",
    "    sample['mp3_q1_SRR'] = eval_output['SRR']\n",
    "    sample['mp3_q1_cr'] = 3*audio.numel()/sum(size_bytes) # 24 bit audio\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19184331-bb99-49be-8621-649b66c2130d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c06dcb8799418fb03f9a30ab52cfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(mp3_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abe3c77-1c33-45bb-b5f2-cc6cd7073211",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dance1_model = RateDistortionAutoEncoder()\n",
    "checkpoint = torch.load(\"dance/audio_stage1_20e.pth\")\n",
    "dance1_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "dance1_model = dance1_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38176de5-a69c-4a12-ae0e-b6d88692621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dance1(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    with torch.no_grad():\n",
    "        compressed = dance1_model.encode(audio.unsqueeze(0).cuda()).round().to(torch.int8).cpu().numpy()\n",
    "        original_shape = compressed.shape\n",
    "        compressed = zlib.compress(compressed.tobytes(),level=9)\n",
    "        size_bytes = len(compressed)\n",
    "        recovered = zlib.decompress(compressed)\n",
    "        recovered = np.frombuffer(recovered, dtype=np.int8)\n",
    "        recovered = recovered.reshape(original_shape)\n",
    "        recovered = torch.tensor(recovered).to(torch.float).cuda()\n",
    "        recovered = dance1_model.decode(recovered).cpu()[0]\n",
    "        eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "        sample['dance1_SSR'] = eval_output['SSR']\n",
    "        sample['dance1_SRR'] = eval_output['SRR']\n",
    "        sample['dance1_cr'] = 3*audio.numel()/size_bytes# 24 bit audio\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d02a25b-a13a-4cc6-8d5a-8492dce3117d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfe394306bb4c90943d678f832b8442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(dance1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231e9741-3f87-4108-a3da-e6517100a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dance2_model = RateDistortionAutoEncoder()\n",
    "checkpoint = torch.load(\"dance/audio_stage2_2e.pth\")\n",
    "dance2_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "dance2_model = dance2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "391fb889-1eb4-47bc-ab58-42419bcbc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dance2(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    with torch.no_grad():\n",
    "        compressed = dance2_model.encode(audio.unsqueeze(0).cuda()).round().to(torch.int8).cpu().numpy()\n",
    "        original_shape = compressed.shape\n",
    "        compressed = zlib.compress(compressed.tobytes(),level=9)\n",
    "        size_bytes = len(compressed)\n",
    "        recovered = zlib.decompress(compressed)\n",
    "        recovered = np.frombuffer(recovered, dtype=np.int8)\n",
    "        recovered = recovered.reshape(original_shape)\n",
    "        recovered = torch.tensor(recovered).to(torch.float).cuda()\n",
    "        recovered = dance2_model.decode(recovered).cpu()[0]\n",
    "        eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "        sample['dance2_SSR'] = eval_output['SSR']\n",
    "        sample['dance2_SRR'] = eval_output['SRR']\n",
    "        sample['dance2_cr'] = 3*audio.numel()/size_bytes# 24 bit audio\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700f080b-ee38-42f7-acde-fc23962d2323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22566d698ba4c5e93b5a591c338f2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(dance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88ff6c64-d4a0-416a-8dee-3d8534848717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp3_q9_SSR: 18.147558212280273 (mean)\n",
      "mp3_q9_SSR: 15.21203327178955 (median) \n",
      "mp3_q9_SRR: 7.068716049194336 (mean)\n",
      "mp3_q9_SRR: 5.95135498046875 (median) \n",
      "mp3_q9_cr: 26.980316162109375 (mean)\n",
      "mp3_q9_cr: 26.646703720092773 (median) \n",
      "mp3_q1_SSR: 34.79498291015625 (mean)\n",
      "mp3_q1_SSR: 28.872365951538086 (median) \n",
      "mp3_q1_SRR: 16.14482879638672 (mean)\n",
      "mp3_q1_SRR: 13.828468322753906 (median) \n",
      "mp3_q1_cr: 8.73799991607666 (mean)\n",
      "mp3_q1_cr: 8.577275276184082 (median) \n",
      "dance1_SSR: 11.579394340515137 (mean)\n",
      "dance1_SSR: 7.241969108581543 (median) \n",
      "dance1_SRR: -0.7877809405326843 (mean)\n",
      "dance1_SRR: 2.2263686656951904 (median) \n",
      "dance1_cr: 1519.7864990234375 (mean)\n",
      "dance1_cr: 63.121192932128906 (median) \n",
      "dance2_SSR: 7.7751288414001465 (mean)\n",
      "dance2_SSR: 4.414035320281982 (median) \n",
      "dance2_SRR: -8.304768562316895 (mean)\n",
      "dance2_SRR: -1.9673932790756226 (median) \n",
      "dance2_cr: 3129.998779296875 (mean)\n",
      "dance2_cr: 177.8354949951172 (median) \n"
     ]
    }
   ],
   "source": [
    "metrics = dataset.remove_columns(['audio','seq_name'])\n",
    "for m in metrics.features:\n",
    "    print(f\"{m}: {metrics[m].mean()} (mean)\")\n",
    "    print(f\"{m}: {metrics[m].median()} (median) \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
