{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b08b213f-5da6-48b2-b0a2-d75998f5e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from io import BytesIO\n",
    "import torch\n",
    "import torchaudio\n",
    "from spauq.core.metrics import spauq_eval\n",
    "import zlib\n",
    "import numpy as np\n",
    "from dance.audio import RateDistortionAutoEncoder\n",
    "from transformers import EncodecModel, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17bb9b35-475c-4efc-a1c7-8f7785abd3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5b2e5dd48f4ec4a75941556710595b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f36d2a1763d49d7a1963c4fcc9ab3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"danjacobellis/aria_ea_audio_preprocessed\",split='validation').with_format(\"torch\").select(range(0,1200,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12a56f1-8c65-4e3f-b506-0e6b220482a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "encodec_model = EncodecModel.from_pretrained(\"facebook/encodec_48khz\")\n",
    "encodec_processor = AutoProcessor.from_pretrained(\"facebook/encodec_48khz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "361f09bf-579e-4d0e-9f7b-66f534ea6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodec48(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    reference = []\n",
    "    recovered = []\n",
    "    size_bytes = []\n",
    "    for c in range(0,7,2):\n",
    "        channels = audio[c:c+2]\n",
    "        if channels.shape[0] != 2:\n",
    "            channels = torch.cat([channels,channels])\n",
    "        with torch.no_grad():\n",
    "            inputs = encodec_processor(raw_audio=channels, sampling_rate=fs, return_tensors='pt')\n",
    "            encoder_outputs = encodec_model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])\n",
    "            size_bytes.append(6.0*10.0*torch.tensor(encoder_outputs.audio_codes.shape).prod().item()/8)\n",
    "            audio_values = encodec_model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]\n",
    "            audio_values = encodec_model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_values\n",
    "            reference.append(inputs['input_values'][0].detach())\n",
    "            recovered.append(audio_values[0].detach())\n",
    "    reference = torch.cat(reference)[0:7]\n",
    "    recovered = torch.cat(recovered)[0:7]\n",
    "    eval_output = spauq_eval(reference=reference,estimate=recovered,fs=fs)\n",
    "    sample['encodec48_SSR'] = eval_output['SSR']\n",
    "    sample['encodec48_SRR'] = eval_output['SRR']\n",
    "    sample['encodec48_cr'] = 3*audio.numel()/sum(size_bytes) # 24 bit audio\n",
    "    sample['encodec48_cr'] = (8/7)*sample['encodec48_cr'] # only 7 channels\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ca6ce0-813c-425f-9fb5-358696b3aa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fa8a05a3ad4ce8977e96464fe358ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/spauq/core/preprocessing.py:325: UserWarning: No forgive_mode specified, defaulting to `none`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(encodec48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d819014e-21a2-4007-a291-c7ea18817ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_q9_stereo(sample):\n",
    "    fs = 48000\n",
    "    audio = sample['audio'].permute(1, 0)\n",
    "    config = torchaudio.io.CodecConfig(qscale=9)\n",
    "    size_bytes = []\n",
    "    recovered = []\n",
    "    for c in range(0, 7, 2):\n",
    "        channels = audio[c:c+2]\n",
    "        if channels.shape[0] != 2:\n",
    "            channels = torch.cat([channels,channels])\n",
    "        \n",
    "        with BytesIO() as f:\n",
    "            torchaudio.save(f, channels, format=\"mp3\", sample_rate=fs, compression=config)\n",
    "            size_bytes.append(len(f.getvalue()))\n",
    "            f.seek(0)\n",
    "            recovered_channels, _ = torchaudio.load(f, format=\"mp3\")\n",
    "            recovered.append(recovered_channels)\n",
    "    recovered = torch.cat(recovered, dim=0)[0:7]\n",
    "    eval_output = spauq_eval(reference=audio, estimate=recovered, fs=fs)\n",
    "    sample['mp3_q9_stereo_SSR'] = eval_output['SSR']\n",
    "    sample['mp3_q9_stereo_SRR'] = eval_output['SRR']\n",
    "    sample['mp3_q9_stereo_cr'] = 3 * audio.numel() / sum(size_bytes)\n",
    "    sample['mp3_q9_stereo_cr'] = (8/7)*sample['mp3_q9_stereo_cr']\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20f89f5b-39d7-4ce1-ad22-ff20146c14f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c9c22651c84bacaa1a9ee8f1f31599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(mp3_q9_stereo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df844d0-e276-4b03-a208-6c997a06a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_q9(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    config = torchaudio.io.CodecConfig(qscale=9)\n",
    "    size_bytes = []\n",
    "    recovered = []\n",
    "    for channel in audio:\n",
    "        with BytesIO() as f:\n",
    "            torchaudio.save(f, channel.unsqueeze(0), format=\"mp3\", sample_rate=fs, compression=config)\n",
    "            size_bytes.append(len(f.getvalue()))\n",
    "            f.seek(0)\n",
    "            recovered.append(torchaudio.load(f,format=\"mp3\")[0])\n",
    "    recovered = torch.cat(recovered)\n",
    "    eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "    sample['mp3_q9_SSR'] = eval_output['SSR']\n",
    "    sample['mp3_q9_SRR'] = eval_output['SRR']\n",
    "    sample['mp3_q9_cr'] = 3*audio.numel()/sum(size_bytes) # 24 bit audio\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a0870c-701a-4a60-b39c-468666e79cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8706763cca5428aa260b31cb9d6dd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/spauq/core/preprocessing.py:325: UserWarning: No forgive_mode specified, defaulting to `none`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(mp3_q9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6548fb-29cd-4162-89a2-c8c4fea5a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_q1(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    config = torchaudio.io.CodecConfig(qscale=1)\n",
    "    size_bytes = []\n",
    "    recovered = []\n",
    "    for channel in audio:\n",
    "        with BytesIO() as f:\n",
    "            torchaudio.save(f, channel.unsqueeze(0), format=\"mp3\", sample_rate=fs, compression=config)\n",
    "            size_bytes.append(len(f.getvalue()))\n",
    "            f.seek(0)\n",
    "            recovered.append(torchaudio.load(f,format=\"mp3\")[0])\n",
    "    recovered = torch.cat(recovered)\n",
    "    eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "    sample['mp3_q1_SSR'] = eval_output['SSR']\n",
    "    sample['mp3_q1_SRR'] = eval_output['SRR']\n",
    "    sample['mp3_q1_cr'] = 3*audio.numel()/sum(size_bytes) # 24 bit audio\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19184331-bb99-49be-8621-649b66c2130d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c06dcb8799418fb03f9a30ab52cfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(mp3_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abe3c77-1c33-45bb-b5f2-cc6cd7073211",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dance1_model = RateDistortionAutoEncoder()\n",
    "checkpoint = torch.load(\"dance/audio_stage1_20e.pth\")\n",
    "dance1_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "dance1_model = dance1_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38176de5-a69c-4a12-ae0e-b6d88692621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dance1(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    with torch.no_grad():\n",
    "        compressed = dance1_model.encode(audio.unsqueeze(0).cuda()).round().to(torch.int8).cpu().numpy()\n",
    "        original_shape = compressed.shape\n",
    "        compressed = zlib.compress(compressed.tobytes(),level=9)\n",
    "        size_bytes = len(compressed)\n",
    "        recovered = zlib.decompress(compressed)\n",
    "        recovered = np.frombuffer(recovered, dtype=np.int8)\n",
    "        recovered = recovered.reshape(original_shape)\n",
    "        recovered = torch.tensor(recovered).to(torch.float).cuda()\n",
    "        recovered = dance1_model.decode(recovered).cpu()[0]\n",
    "        eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "        sample['dance1_SSR'] = eval_output['SSR']\n",
    "        sample['dance1_SRR'] = eval_output['SRR']\n",
    "        sample['dance1_cr'] = 3*audio.numel()/size_bytes# 24 bit audio\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d02a25b-a13a-4cc6-8d5a-8492dce3117d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfe394306bb4c90943d678f832b8442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(dance1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231e9741-3f87-4108-a3da-e6517100a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dance2_model = RateDistortionAutoEncoder()\n",
    "checkpoint = torch.load(\"dance/audio_stage2_2e.pth\")\n",
    "dance2_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "dance2_model = dance2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "391fb889-1eb4-47bc-ab58-42419bcbc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dance2(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    with torch.no_grad():\n",
    "        compressed = dance2_model.encode(audio.unsqueeze(0).cuda()).round().to(torch.int8).cpu().numpy()\n",
    "        original_shape = compressed.shape\n",
    "        compressed = zlib.compress(compressed.tobytes(),level=9)\n",
    "        size_bytes = len(compressed)\n",
    "        recovered = zlib.decompress(compressed)\n",
    "        recovered = np.frombuffer(recovered, dtype=np.int8)\n",
    "        recovered = recovered.reshape(original_shape)\n",
    "        recovered = torch.tensor(recovered).to(torch.float).cuda()\n",
    "        recovered = dance2_model.decode(recovered).cpu()[0]\n",
    "        eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "        sample['dance2_SSR'] = eval_output['SSR']\n",
    "        sample['dance2_SRR'] = eval_output['SRR']\n",
    "        sample['dance2_cr'] = 3*audio.numel()/size_bytes# 24 bit audio\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700f080b-ee38-42f7-acde-fc23962d2323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22566d698ba4c5e93b5a591c338f2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(dance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88ff6c64-d4a0-416a-8dee-3d8534848717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodec48_SSR: 2.7921228408813477 (mean)\n",
      "encodec48_SSR: 1.3160523176193237 (median) \n",
      "encodec48_SRR: -11.988890647888184 (mean)\n",
      "encodec48_SRR: -12.811376571655273 (median) \n",
      "encodec48_cr: 114.28572845458984 (mean)\n",
      "encodec48_cr: 114.28571319580078 (median) \n",
      "mp3_q9_stereo_SSR: 18.158334732055664 (mean)\n",
      "mp3_q9_stereo_SSR: 14.851905822753906 (median) \n",
      "mp3_q9_stereo_SRR: 7.02269172668457 (mean)\n",
      "mp3_q9_stereo_SRR: 5.88826847076416 (median) \n",
      "mp3_q9_stereo_cr: 31.54193115234375 (mean)\n",
      "mp3_q9_stereo_cr: 31.690698623657227 (median) \n"
     ]
    }
   ],
   "source": [
    "metrics = dataset.remove_columns(['audio','seq_name'])\n",
    "for m in metrics.features:\n",
    "    print(f\"{m}: {metrics[m].mean()} (mean)\")\n",
    "    print(f\"{m}: {metrics[m].median()} (median) \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
