{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b08b213f-5da6-48b2-b0a2-d75998f5e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from io import BytesIO\n",
    "import torch\n",
    "import torchaudio\n",
    "from spauq.core.metrics import spauq_eval\n",
    "import zlib\n",
    "import numpy as np\n",
    "from dance.audio import RateDistortionAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17bb9b35-475c-4efc-a1c7-8f7785abd3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f1d9dbed5443888fe191bbd30b0a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9bb72fe81e4e39891380618644b200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"danjacobellis/aria_ea_audio_preprocessed\",split='validation').with_format(\"torch\").select(range(0,1200,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df844d0-e276-4b03-a208-6c997a06a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_q9(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    config = torchaudio.io.CodecConfig(qscale=9)\n",
    "    size_bytes = []\n",
    "    recovered = []\n",
    "    for channel in audio:\n",
    "        with BytesIO() as f:\n",
    "            torchaudio.save(f, channel.unsqueeze(0), format=\"mp3\", sample_rate=fs, compression=config)\n",
    "            size_bytes.append(len(f.getvalue()))\n",
    "            f.seek(0)\n",
    "            recovered.append(torchaudio.load(f,format=\"mp3\")[0])\n",
    "    recovered = torch.cat(recovered)\n",
    "    eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "    sample['mp3_q9_SSR'] = eval_output['SSR']\n",
    "    sample['mp3_q9_SRR'] = eval_output['SRR']\n",
    "    sample['mp3_q9_cr'] = 3*audio.numel()/sum(size_bytes) # 24 bit audio\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a0870c-701a-4a60-b39c-468666e79cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8706763cca5428aa260b31cb9d6dd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/spauq/core/preprocessing.py:325: UserWarning: No forgive_mode specified, defaulting to `none`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(mp3_q9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6548fb-29cd-4162-89a2-c8c4fea5a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_q1(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    config = torchaudio.io.CodecConfig(qscale=1)\n",
    "    size_bytes = []\n",
    "    recovered = []\n",
    "    for channel in audio:\n",
    "        with BytesIO() as f:\n",
    "            torchaudio.save(f, channel.unsqueeze(0), format=\"mp3\", sample_rate=fs, compression=config)\n",
    "            size_bytes.append(len(f.getvalue()))\n",
    "            f.seek(0)\n",
    "            recovered.append(torchaudio.load(f,format=\"mp3\")[0])\n",
    "    recovered = torch.cat(recovered)\n",
    "    eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "    sample['mp3_q1_SSR'] = eval_output['SSR']\n",
    "    sample['mp3_q1_SRR'] = eval_output['SRR']\n",
    "    sample['mp3_q1_cr'] = 3*audio.numel()/sum(size_bytes) # 24 bit audio\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19184331-bb99-49be-8621-649b66c2130d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c06dcb8799418fb03f9a30ab52cfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(mp3_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abe3c77-1c33-45bb-b5f2-cc6cd7073211",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dance1_model = RateDistortionAutoEncoder()\n",
    "checkpoint = torch.load(\"dance/audio_stage1_20e.pth\")\n",
    "dance1_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "dance1_model = dance1_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38176de5-a69c-4a12-ae0e-b6d88692621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dance1(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    with torch.no_grad():\n",
    "        compressed = dance1_model.encode(audio.unsqueeze(0).cuda()).round().to(torch.int8).cpu().numpy()\n",
    "        original_shape = compressed.shape\n",
    "        compressed = zlib.compress(compressed.tobytes(),level=9)\n",
    "        size_bytes = len(compressed)\n",
    "        recovered = zlib.decompress(compressed)\n",
    "        recovered = np.frombuffer(recovered, dtype=np.int8)\n",
    "        recovered = recovered.reshape(original_shape)\n",
    "        recovered = torch.tensor(recovered).to(torch.float).cuda()\n",
    "        recovered = dance1_model.decode(recovered).cpu()[0]\n",
    "        eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "        sample['dance1_SSR'] = eval_output['SSR']\n",
    "        sample['dance1_SRR'] = eval_output['SRR']\n",
    "        sample['dance1_cr'] = 3*audio.numel()/size_bytes# 24 bit audio\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d02a25b-a13a-4cc6-8d5a-8492dce3117d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfe394306bb4c90943d678f832b8442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(dance1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231e9741-3f87-4108-a3da-e6517100a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dance2_model = RateDistortionAutoEncoder()\n",
    "checkpoint = torch.load(\"dance/audio_stage2_2e.pth\")\n",
    "dance2_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "dance2_model = dance2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "391fb889-1eb4-47bc-ab58-42419bcbc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dance2(sample):\n",
    "    fs=48000\n",
    "    audio = sample['audio'].permute(1,0)\n",
    "    with torch.no_grad():\n",
    "        compressed = dance2_model.encode(audio.unsqueeze(0).cuda()).round().to(torch.int8).cpu().numpy()\n",
    "        original_shape = compressed.shape\n",
    "        compressed = zlib.compress(compressed.tobytes(),level=9)\n",
    "        size_bytes = len(compressed)\n",
    "        recovered = zlib.decompress(compressed)\n",
    "        recovered = np.frombuffer(recovered, dtype=np.int8)\n",
    "        recovered = recovered.reshape(original_shape)\n",
    "        recovered = torch.tensor(recovered).to(torch.float).cuda()\n",
    "        recovered = dance2_model.decode(recovered).cpu()[0]\n",
    "        eval_output = spauq_eval(reference=audio,estimate=recovered,fs=fs)\n",
    "        sample['dance2_SSR'] = eval_output['SSR']\n",
    "        sample['dance2_SRR'] = eval_output['SRR']\n",
    "        sample['dance2_cr'] = 3*audio.numel()/size_bytes# 24 bit audio\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700f080b-ee38-42f7-acde-fc23962d2323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22566d698ba4c5e93b5a591c338f2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(dance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88ff6c64-d4a0-416a-8dee-3d8534848717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp3_q9_SSR: 18.147558212280273 (mean)\n",
      "mp3_q9_SSR: 15.21203327178955 (median) \n",
      "mp3_q9_SRR: 7.068716049194336 (mean)\n",
      "mp3_q9_SRR: 5.95135498046875 (median) \n",
      "mp3_q9_cr: 26.980316162109375 (mean)\n",
      "mp3_q9_cr: 26.646703720092773 (median) \n",
      "mp3_q1_SSR: 34.79498291015625 (mean)\n",
      "mp3_q1_SSR: 28.872365951538086 (median) \n",
      "mp3_q1_SRR: 16.14482879638672 (mean)\n",
      "mp3_q1_SRR: 13.828468322753906 (median) \n",
      "mp3_q1_cr: 8.73799991607666 (mean)\n",
      "mp3_q1_cr: 8.577275276184082 (median) \n",
      "dance1_SSR: 11.579394340515137 (mean)\n",
      "dance1_SSR: 7.241969108581543 (median) \n",
      "dance1_SRR: -0.7877809405326843 (mean)\n",
      "dance1_SRR: 2.2263686656951904 (median) \n",
      "dance1_cr: 1519.7864990234375 (mean)\n",
      "dance1_cr: 63.121192932128906 (median) \n",
      "dance2_SSR: 7.7751288414001465 (mean)\n",
      "dance2_SSR: 4.414035320281982 (median) \n",
      "dance2_SRR: -8.304768562316895 (mean)\n",
      "dance2_SRR: -1.9673932790756226 (median) \n",
      "dance2_cr: 3129.998779296875 (mean)\n",
      "dance2_cr: 177.8354949951172 (median) \n"
     ]
    }
   ],
   "source": [
    "metrics = dataset.remove_columns(['audio','seq_name'])\n",
    "for m in metrics.features:\n",
    "    print(f\"{m}: {metrics[m].mean()} (mean)\")\n",
    "    print(f\"{m}: {metrics[m].median()} (median) \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
