{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495d367f-ec8f-4757-9fd6-ef8c78b43740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from IPython.display import display\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import PILToTensor, ToTensor, ToPILImage\n",
    "from dance.image import compress, decompress, preprocess, postprocess, RateDistortionAutoEncoder\n",
    "import compressai\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170cf7fc-3244-49d0-96a9-5191b634498e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66776ca6a87f40f59349f733edb74b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754031c26b91493595ff7e3b83080e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6f5009f4974c63aa72c75578d9a0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98b99c7b801418e80d08997893bd12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4996fa6c387448e94b4c5cc57bc8845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").cuda()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "dataset = load_dataset(\"danjacobellis/aria_ea_rgb_100k\",split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c94fad0-3b3c-408d-895d-166a225c2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"kitchen\",\n",
    "    \"cooking\",\n",
    "    \"food\",\n",
    "    \"drink\",\n",
    "    \"spill\",\n",
    "    \"table\",\n",
    "    \"television\",\n",
    "    \"phone\",\n",
    "    \"laptop\",\n",
    "    \"video game\",\n",
    "    \"board game\",\n",
    "    \"clothes\",\n",
    "    \"laundry\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3829a0be-05f0-44c9-9b3a-d6390a5ae9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip(sample):\n",
    "    with torch.no_grad():\n",
    "        image = sample['image']\n",
    "        inputs = clip_processor(text=classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "        for k in inputs.keys():\n",
    "            if hasattr(inputs[k], \"device\"):\n",
    "                inputs[k] = inputs[k].cuda()\n",
    "        outputs = clip_model(**inputs)\n",
    "        sample['clip_logit'] = outputs.logits_per_image\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295e257-489f-4f29-8d3b-ed9aae1001f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dataset = dataset.map(get_clip);\n",
    "clip_logit = clip_dataset.with_format(\"torch\")['clip_logit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cce2e7-8884-49b3-96e4-195c6479fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jpeg_compress(img, rr=3, subsampling=0, quality=5):\n",
    "    w = img.width\n",
    "    h = img.height\n",
    "    with BytesIO() as f:\n",
    "        img = img.resize((w//rr,h//rr))\n",
    "        img = img.save(f,\n",
    "                       format='JPEG',\n",
    "                       subsampling=subsampling,\n",
    "                       quality=quality\n",
    "                      )\n",
    "        img = f.getvalue()\n",
    "    bpp = 8*len(img)/(w*h)\n",
    "    img = Image.open(BytesIO(img)).resize((w,h))\n",
    "    return img,bpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99614704-bcca-4cb5-b60a-06d4b9de0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_jpeg(sample):\n",
    "    with torch.no_grad():\n",
    "        image, bpp = jpeg_compress(sample['image'])\n",
    "        inputs = clip_processor(text=classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "        for k in inputs.keys():\n",
    "            if hasattr(inputs[k], \"device\"):\n",
    "                inputs[k] = inputs[k].cuda()\n",
    "        outputs = clip_model(**inputs)\n",
    "        sample['clip_logit'] = outputs.logits_per_image\n",
    "        sample['bpp'] = bpp\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829bd85-f3e1-4bb5-b634-26ed82288b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_dataset = dataset.map(get_clip_jpeg)\n",
    "jpeg_clip_logit = jpeg_dataset.with_format(\"torch\")['clip_logit']\n",
    "jpeg_bpp = jpeg_dataset.with_format(\"torch\")['bpp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f633b-95c5-4915-a904-cd7f7f2a29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dance_compress(img,device):\n",
    "    with torch.no_grad():\n",
    "        img = PILToTensor()(img).permute(1,2,0)\n",
    "        batch = preprocess(img,device)\n",
    "        compressed_img, original_shape = compress(batch, dance_model)\n",
    "        bpp = 8*len(compressed_img)/(batch.shape[2]*batch.shape[3])\n",
    "        rec = decompress(compressed_img, original_shape, dance_model)\n",
    "        return postprocess(rec),bpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35a6af-b35e-44be-a601-e7d780cd50dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_dance(sample):\n",
    "    with torch.no_grad():\n",
    "        image, bpp = dance_compress(sample['image'],dance_model.device)\n",
    "        inputs = clip_processor(text=classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "        for k in inputs.keys():\n",
    "            if hasattr(inputs[k], \"device\"):\n",
    "                inputs[k] = inputs[k].cuda()\n",
    "        outputs = clip_model(**inputs)\n",
    "        sample['clip_logit'] = outputs.logits_per_image\n",
    "        sample['bpp'] = bpp\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04138c-b964-489c-85dd-8133cc3fc858",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dance_model = RateDistortionAutoEncoder()\n",
    "checkpoint = torch.load(\"dance/image.pth\")\n",
    "dance_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "dance_model = dance_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca800765-edb5-4f08-b01c-916638d58e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dance_dataset = dataset.map(get_clip_dance)\n",
    "dance_clip_logit = dance_dataset.with_format(\"torch\")['clip_logit']\n",
    "dance_bpp = dance_dataset.with_format(\"torch\")['bpp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f7ff9-819b-4022-b87a-d736f19c13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgml_compress(img,model,device):\n",
    "    w = img.width\n",
    "    h = img.height\n",
    "    \n",
    "    if (img.mode == 'L') | (img.mode == 'CMYK') | (img.mode == 'RGBA'):\n",
    "        rgbimg = PIL.Image.new(\"RGB\", img.size)\n",
    "        rgbimg.paste(img)\n",
    "        img = rgbimg\n",
    "    \n",
    "    x = ToTensor()(img).unsqueeze(0)\n",
    "    orig_size = (x.size(2), x.size(3))\n",
    "    x = x.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = model.g_a(x)\n",
    "        compressed = z.to(torch.int8).detach().cpu().numpy()\n",
    "        num_bytes = len(zlib.compress(compressed.tobytes(), level=9))\n",
    "        recovered  = model.g_s(z)\n",
    "    recovered.clamp_(0, 1);\n",
    "    img = ToPILImage()(recovered.squeeze())\n",
    "    bpp = 8*num_bytes/(w*h)\n",
    "    return img,bpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a143c-46d6-4d46-a36a-d370b624d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_dgml(sample):\n",
    "    with torch.no_grad():\n",
    "        image, bpp = dgml_compress(sample['image'],model=dgml_model,device=\"cuda\")\n",
    "        inputs = clip_processor(text=classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "        for k in inputs.keys():\n",
    "            if hasattr(inputs[k], \"device\"):\n",
    "                inputs[k] = inputs[k].cuda()\n",
    "        outputs = clip_model(**inputs)\n",
    "        sample['clip_logit'] = outputs.logits_per_image\n",
    "        sample['bpp'] = bpp\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e8815-ed17-4864-aaae-b638f858fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dgml_model = compressai.zoo.cheng2020_attn(quality=1, pretrained=True)\n",
    "dgml_model = dgml_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c173dd0c-ba0d-410d-a709-db5947c26b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgml_dataset = dataset.map(get_clip_dgml)\n",
    "dgml_clip_logit = dgml_dataset.with_format(\"torch\")['clip_logit']\n",
    "dgml_bpp = dgml_dataset.with_format(\"torch\")['bpp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd6a6e-2772-4af5-b044-105195d3e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = Dataset.from_dict(\n",
    "    {\"jpeg_bpp\": jpeg_bpp,\n",
    "     \"dance_bpp\": dance_bpp,\n",
    "     \"dgml_bpp\": dgml_bpp,\n",
    "     **{c: clip_logit[:, 0, i] for i, c in enumerate(classes)},\n",
    "     **{\"jpeg_\" + c: jpeg_clip_logit[:, 0, i] for i, c in enumerate(classes)},\n",
    "     **{\"dance_\" + c: dance_clip_logit[:, 0, i] for i, c in enumerate(classes)},\n",
    "     **{\"dgml_\" + c: dgml_clip_logit[:, 0, i] for i, c in enumerate(classes)}})\n",
    "combined_dataset.push_to_hub(\"danjacobellis/area_compression\",split=\"validation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
